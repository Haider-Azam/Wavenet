{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import h5py\n",
    "from torch.utils.data import DataLoader\n",
    "import pandas as pd\n",
    "import skorch\n",
    "from skorch.dataset import Dataset\n",
    "from skorch.callbacks import Checkpoint,ProgressBar\n",
    "from skorch.helper import predefined_split\n",
    "from configTUHdl import *\n",
    "from dataset import *\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from mne import set_log_level\n",
    "import resampy\n",
    "from skorch.callbacks import LRScheduler\n",
    "from sklearn.metrics import classification_report\n",
    "from WavenetLSTM import WavenetLSTM\n",
    "set_log_level(False)\n",
    "device = 'cuda' if cuda else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ch_names=['A1', 'A2', 'C3', 'C4', 'CZ', 'F3', 'F4', 'F7', 'F8', 'FP1','FP2', 'FZ',\n",
    "               'O1', 'O2','P3', 'P4', 'PZ', 'T3', 'T4', 'T5', 'T6']\n",
    "#Implementing Transverse Central Parietal (TCP) montage technique for a single sample\n",
    "def tcp(data,fs):\n",
    "    length=data.shape[1]\n",
    "    new_data =np.zeros(shape=(20,length),dtype=np.float32)\n",
    "    \n",
    "    new_data[0] = (data[ch_names.index('FP1'),:]) - (data[ch_names.index('F7'),:])\n",
    "    new_data[1] = (data[ch_names.index('FP2'),:]) - (data[ch_names.index('F8'),:])\n",
    "    new_data[2] = (data[ch_names.index('F7'),:]) - (data[ch_names.index('T3'),:])\n",
    "    new_data[3] = (data[ch_names.index('F8'),:]) - (data[ch_names.index('T4'),:])\n",
    "    new_data[4] = (data[ch_names.index('T3'),:]) - (data[ch_names.index('T5'),:])\n",
    "    new_data[5] = (data[ch_names.index('T4'),:]) - (data[ch_names.index('T6'),:])\n",
    "    new_data[6] = (data[ch_names.index('T5'),:]) - (data[ch_names.index('O1'),:])\n",
    "    new_data[7] = (data[ch_names.index('T6'),:]) - (data[ch_names.index('O2'),:])\n",
    "    new_data[8] = (data[ch_names.index('T3'),:]) - (data[ch_names.index('C3'),:])\n",
    "    new_data[9] = (data[ch_names.index('T4'),:]) - (data[ch_names.index('C4'),:])\n",
    "    new_data[10] = (data[ch_names.index('C3'),:]) - (data[ch_names.index('CZ'),:])\n",
    "    new_data[11] = (data[ch_names.index('CZ'),:]) - (data[ch_names.index('C4'),:])\n",
    "    new_data[12] = (data[ch_names.index('FP1'),:]) - (data[ch_names.index('F3'),:])\n",
    "    new_data[13] = (data[ch_names.index('FP2'),:]) - (data[ch_names.index('F4'),:])\n",
    "    new_data[14] = (data[ch_names.index('F3'),:]) - (data[ch_names.index('C3'),:])\n",
    "    new_data[15] = (data[ch_names.index('F4'),:]) - (data[ch_names.index('C4'),:])\n",
    "    new_data[16] = (data[ch_names.index('C3'),:]) - (data[ch_names.index('P3'),:])\n",
    "    new_data[17] = (data[ch_names.index('C4'),:]) - (data[ch_names.index('P4'),:])\n",
    "    new_data[18] = (data[ch_names.index('P3'),:]) - (data[ch_names.index('O1'),:])\n",
    "    new_data[19] = (data[ch_names.index('P4'),:]) - (data[ch_names.index('O2'),:])\n",
    "\n",
    "    return new_data, fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import butter,iirnotch,filtfilt\n",
    "butter_b,butter_a=butter(4,1,btype='highpass',fs=sampling_freq)\n",
    "notch_b,notch_a=iirnotch(60,Q=30,fs=sampling_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "preproc_functions = []\n",
    "#Cut to 2 minutes length\n",
    "preproc_functions.append(lambda data, fs: (data[:, :int(duration_recording_mins * 60 * fs)], fs))\n",
    "#Apply butterworth and notch filter\n",
    "preproc_functions.append(lambda data, fs: (filtfilt(butter_b, butter_a, data), fs))\n",
    "preproc_functions.append(lambda data, fs: (filtfilt(notch_b, notch_a, data), fs))\n",
    "#Apply TCP montage technique\n",
    "preproc_functions.append(tcp)\n",
    "preproc_functions.append(lambda data, fs: (resampy.resample(data, fs,sampling_freq,axis=1,filter='kaiser_fast'),sampling_freq))\n",
    "\n",
    "dataset = DiagnosisSet(n_recordings=n_recordings,\n",
    "                           max_recording_mins=max_recording_mins,\n",
    "                           preproc_functions=preproc_functions,\n",
    "                           data_folders=data_folders,\n",
    "                           train_or_eval='train',\n",
    "                           sensor_types=sensor_types)\n",
    "if test_on_eval:\n",
    "    test_dataset = DiagnosisSet(n_recordings=n_recordings,\n",
    "                           max_recording_mins=max_recording_mins,\n",
    "                           preproc_functions=preproc_functions,\n",
    "                           data_folders=data_folders,\n",
    "                           train_or_eval='eval',\n",
    "                           sensor_types=sensor_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y=dataset.load()\n",
    "test_x,test_y=test_dataset.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data=np.array(X)\n",
    "test_data=np.array(test_x)\n",
    "del X,test_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_train_data=np.concatenate([train_data[:,:,:input_time_length] , train_data[:,:,-1:input_time_length-1:-1]])\n",
    "augmented_test_data=np.concatenate([test_data[:,:,:input_time_length] , test_data[:,:,-1:input_time_length-1:-1]])\n",
    "\n",
    "augmented_train_label=np.concatenate([y,y])\n",
    "augmented_test_label=np.concatenate([test_y,test_y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#HDF5 implementation\n",
    "file_names=[]\n",
    "split='train'\n",
    "path=os.path.join(processed_folder,split)\n",
    "for i in range(len(augmented_train_label)):\n",
    "    file_path=f'{path}/{i}.hdf5'\n",
    "    file_names.append(file_path)\n",
    "    with h5py.File(file_path, 'a') as f:\n",
    "        f['x']=augmented_train_data[i]\n",
    "        f['y']=augmented_train_label[i]\n",
    "    #np.savez_compressed(file_path,x=X[i],y=y[i])\n",
    "file_names=pd.Series(file_names)\n",
    "lbs=pd.Series(augmented_train_label)\n",
    "train_dataframe=pd.DataFrame({'name':file_names,'label':lbs})\n",
    "train_dataframe.to_excel(f\"{processed_folder}/{split}.xlsx\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_names=[]\n",
    "split='eval'\n",
    "path=os.path.join(processed_folder,split)\n",
    "for i in range(len(augmented_test_label)):\n",
    "    file_path=f'{path}/{i}.hdf5'\n",
    "    file_names.append(file_path)\n",
    "    with h5py.File(file_path, 'a') as f:\n",
    "        f['x']=augmented_test_data[i]\n",
    "        f['y']=augmented_test_label[i]\n",
    "\n",
    "file_names=pd.Series(file_names)\n",
    "lbs=pd.Series(augmented_test_label)\n",
    "eval_dataframe=pd.DataFrame({'name':file_names,'label':lbs})\n",
    "eval_dataframe.to_excel(f\"{processed_folder}/{split}.xlsx\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WindowDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, excel_path):\n",
    "        super().__init__()\n",
    "        excel_file=pd.read_excel(excel_path)\n",
    "        self.file_names=excel_file['name'].to_numpy(dtype=str)\n",
    "        self.label=excel_file['label'].to_numpy()\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        with h5py.File(self.file_names[index], 'r') as h5_file:\n",
    "            window=np.array(h5_file['x'])\n",
    "\n",
    "        label=self.label[index]\n",
    "        return window,label\n",
    " \n",
    "    def __len__(self):\n",
    "        return len(self.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The train and test set need to be initialized here\n",
    "test_set=WindowDataset(f'{processed_folder}/eval.xlsx')\n",
    "train_set=WindowDataset(f'{processed_folder}/train.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[  0.17097712,   1.3287303 ,   0.7692792 , ...,  -5.556323  ,\n",
       "          24.01627   ,  -4.044311  ],\n",
       "        [ -3.3584309 ,   0.09836197,  -5.3884964 , ...,  -8.297503  ,\n",
       "         -17.203846  ,  -1.3521657 ],\n",
       "        [  1.4771857 ,  -5.01991   ,  -2.4995058 , ..., -10.169931  ,\n",
       "          -9.443199  ,   2.1237545 ],\n",
       "        ...,\n",
       "        [ -1.6094382 ,  -0.13247037,   0.74053544, ...,   2.6964524 ,\n",
       "          -1.2182155 ,   2.372343  ],\n",
       "        [ -1.5516642 ,  -3.4095576 ,  -2.2637873 , ...,   7.82981   ,\n",
       "           8.16312   ,   8.779852  ],\n",
       "        [ -2.2247756 ,  -3.852793  ,  -4.484772  , ...,  -0.29596758,\n",
       "           1.6410568 ,   2.8635814 ]], dtype=float32),\n",
       " 1)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set.__getitem__(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name='wavenet'\n",
    "criterion=torch.nn.CrossEntropyLoss\n",
    "optimizer_lr=0.0005\n",
    "n_chans=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=WavenetLSTM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 6\u001b[0m\n\u001b[0;32m      2\u001b[0m cp\u001b[38;5;241m=\u001b[39mCheckpoint(monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalid_f1_best\u001b[39m\u001b[38;5;124m'\u001b[39m,dirname\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m,f_params\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mbest_param.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m      3\u001b[0m                f_optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mbest_opt.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m, f_history\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mbest_history.json\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      4\u001b[0m path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      5\u001b[0m classifier \u001b[38;5;241m=\u001b[39m skorch\u001b[38;5;241m.\u001b[39mNeuralNetClassifier(\n\u001b[1;32m----> 6\u001b[0m         \u001b[43mmodel\u001b[49m,\n\u001b[0;32m      7\u001b[0m         optimizer\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdamW,\n\u001b[0;32m      8\u001b[0m         train_split\u001b[38;5;241m=\u001b[39mpredefined_split(test_set),\n\u001b[0;32m      9\u001b[0m         optimizer__lr\u001b[38;5;241m=\u001b[39moptimizer_lr,\n\u001b[0;32m     10\u001b[0m         \u001b[38;5;66;03m#optimizer__weight_decay=optimizer_weight_decay,\u001b[39;00m\n\u001b[0;32m     11\u001b[0m         iterator_train\u001b[38;5;241m=\u001b[39mDataLoader,\n\u001b[0;32m     12\u001b[0m         iterator_valid\u001b[38;5;241m=\u001b[39mDataLoader,\n\u001b[0;32m     13\u001b[0m         iterator_train__shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     14\u001b[0m         iterator_train__pin_memory\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     15\u001b[0m         iterator_valid__pin_memory\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     16\u001b[0m         \u001b[38;5;66;03m#iterator_train__num_workers=1,\u001b[39;00m\n\u001b[0;32m     17\u001b[0m         \u001b[38;5;66;03m#iterator_valid__num_workers=1,\u001b[39;00m\n\u001b[0;32m     18\u001b[0m         \u001b[38;5;66;03m#iterator_train__persistent_workers=True,\u001b[39;00m\n\u001b[0;32m     19\u001b[0m         \u001b[38;5;66;03m#iterator_valid__persistent_workers=True,\u001b[39;00m\n\u001b[0;32m     20\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[0;32m     21\u001b[0m         device\u001b[38;5;241m=\u001b[39mdevice,\n\u001b[0;32m     22\u001b[0m         \u001b[38;5;66;03m#callbacks=[\"accuracy\",\"f1\",cp,ProgressBar(detect_notebook=True)],\u001b[39;00m\n\u001b[0;32m     23\u001b[0m         warm_start\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     24\u001b[0m         )\n\u001b[0;32m     25\u001b[0m classifier\u001b[38;5;241m.\u001b[39minitialize()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "monitor = lambda net: any(net.history[-1, ('valid_accuracy_best','valid_f1_best','valid_loss_best')])\n",
    "cp=Checkpoint(monitor='valid_f1_best',dirname='model',f_params=f'{model_name}best_param.pkl',\n",
    "               f_optimizer=f'{model_name}best_opt.pkl', f_history=f'{model_name}best_history.json')\n",
    "path=f'{model_name}'\n",
    "classifier = skorch.NeuralNetClassifier(\n",
    "        model,\n",
    "        optimizer=torch.optim.AdamW,\n",
    "        train_split=predefined_split(test_set),\n",
    "        optimizer__lr=optimizer_lr,\n",
    "        #optimizer__weight_decay=optimizer_weight_decay,\n",
    "        iterator_train=DataLoader,\n",
    "        iterator_valid=DataLoader,\n",
    "        iterator_train__shuffle=True,\n",
    "        iterator_train__pin_memory=True,\n",
    "        iterator_valid__pin_memory=True,\n",
    "        #iterator_train__num_workers=1,\n",
    "        #iterator_valid__num_workers=1,\n",
    "        #iterator_train__persistent_workers=True,\n",
    "        #iterator_valid__persistent_workers=True,\n",
    "        batch_size=batch_size,\n",
    "        device=device,\n",
    "        #callbacks=[\"accuracy\",\"f1\",cp,ProgressBar(detect_notebook=True)],\n",
    "        warm_start=True,\n",
    "        )\n",
    "classifier.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test=np.random.rand(3,n_chans,input_time_length)\n",
    "out=classifier.predict(test)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Used to load parameters for ongoing training\n",
    "try:\n",
    "    classifier.load_params(\n",
    "        f_params=f'model/{path}_param.pkl', f_optimizer=f'model/{path}_opt.pkl', f_history=f'model/{path}_history.json')\n",
    "    print(\"Paramters Loaded\")\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.fit(train_set,y=None,epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.save_params(\n",
    "    f_params=f'model/{path}_param.pkl', f_optimizer=f'model/{path}_opt.pkl', f_history=f'model/{path}_history.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples=[]\n",
    "for i in range(test_set.__len__()):\n",
    "    samples.append(test_set.__getitem__(i)[0])\n",
    "samples=np.array(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample=test_set.__getitem__(0)[0][None,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paramters Loaded\n"
     ]
    }
   ],
   "source": [
    "classifier.load_params(\n",
    "        f_params=f'model/{model_name}best_param.pkl', f_history=f'model/{model_name}best_history.json')\n",
    "print(\"Paramters Loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "out=classifier.predict(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.load_params(\n",
    "        f_params=f'model/{model_name}best_param.pkl', f_history=f'model/{model_name}best_history.json')\n",
    "print(\"Paramters Loaded\")\n",
    "pred_labels=classifier.predict(test_set)\n",
    "actual_labels=[label[1] for label in test_set]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lenovo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\lenovo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\lenovo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "report=classification_report(actual_labels,pred_labels,target_names=['false','true'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc=roc_auc_score(actual_labels,classifier.predict_proba(test_set)[:,1])\n",
    "actual_labels=np.array(actual_labels)\n",
    "accuracy=np.mean(pred_labels==actual_labels)\n",
    "tp=np.sum(pred_labels*actual_labels)\n",
    "precision=tp/np.sum(pred_labels)\n",
    "recall=tp/np.sum(actual_labels)\n",
    "f1=2*precision*recall/(precision+recall)\n",
    "\n",
    "print(model_name)\n",
    "print(f\"Accuracy:{accuracy}\")\n",
    "print(f\"F1-Score:{f1}\")\n",
    "print(f\"roc_auc score:{auc}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
