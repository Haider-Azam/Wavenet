{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow not install, you could not use those pipelines\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import h5py\n",
    "from torch.utils.data import DataLoader\n",
    "import pandas as pd\n",
    "import skorch\n",
    "from skorch.dataset import Dataset\n",
    "from skorch.callbacks import Checkpoint,ProgressBar\n",
    "from skorch.helper import predefined_split\n",
    "from configTUHdl import *\n",
    "from dataset import *\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from mne import set_log_level\n",
    "import resampy\n",
    "from skorch.callbacks import LRScheduler\n",
    "set_log_level(False)\n",
    "device = 'cuda' if cuda else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "ch_names=['A1', 'A2', 'C3', 'C4', 'CZ', 'F3', 'F4', 'F7', 'F8', 'FP1','FP2', 'FZ',\n",
    "               'O1', 'O2','P3', 'P4', 'PZ', 'T3', 'T4', 'T5', 'T6']\n",
    "#Implementing Transverse Central Parietal (TCP) montage technique for a single sample\n",
    "def tcp(data,fs):\n",
    "    length=data.shape[1]\n",
    "    new_data =np.zeros(shape=(20,length),dtype=np.float32)\n",
    "    \n",
    "    new_data[0] = (data[ch_names.index('FP1'),:]) - (data[ch_names.index('F7'),:])\n",
    "    new_data[1] = (data[ch_names.index('FP2'),:]) - (data[ch_names.index('F8'),:])\n",
    "    new_data[2] = (data[ch_names.index('F7'),:]) - (data[ch_names.index('T3'),:])\n",
    "    new_data[3] = (data[ch_names.index('F8'),:]) - (data[ch_names.index('T4'),:])\n",
    "    new_data[4] = (data[ch_names.index('T3'),:]) - (data[ch_names.index('T5'),:])\n",
    "    new_data[5] = (data[ch_names.index('T4'),:]) - (data[ch_names.index('T6'),:])\n",
    "    new_data[6] = (data[ch_names.index('T5'),:]) - (data[ch_names.index('O1'),:])\n",
    "    new_data[7] = (data[ch_names.index('T6'),:]) - (data[ch_names.index('O2'),:])\n",
    "    new_data[8] = (data[ch_names.index('T3'),:]) - (data[ch_names.index('C3'),:])\n",
    "    new_data[9] = (data[ch_names.index('T4'),:]) - (data[ch_names.index('C4'),:])\n",
    "    new_data[10] = (data[ch_names.index('C3'),:]) - (data[ch_names.index('CZ'),:])\n",
    "    new_data[11] = (data[ch_names.index('CZ'),:]) - (data[ch_names.index('C4'),:])\n",
    "    new_data[12] = (data[ch_names.index('FP1'),:]) - (data[ch_names.index('F3'),:])\n",
    "    new_data[13] = (data[ch_names.index('FP2'),:]) - (data[ch_names.index('F4'),:])\n",
    "    new_data[14] = (data[ch_names.index('F3'),:]) - (data[ch_names.index('C3'),:])\n",
    "    new_data[15] = (data[ch_names.index('F4'),:]) - (data[ch_names.index('C4'),:])\n",
    "    new_data[16] = (data[ch_names.index('C3'),:]) - (data[ch_names.index('P3'),:])\n",
    "    new_data[17] = (data[ch_names.index('C4'),:]) - (data[ch_names.index('P4'),:])\n",
    "    new_data[18] = (data[ch_names.index('P3'),:]) - (data[ch_names.index('O1'),:])\n",
    "    new_data[19] = (data[ch_names.index('P4'),:]) - (data[ch_names.index('O2'),:])\n",
    "\n",
    "    return new_data, fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import butter,iirnotch,filtfilt\n",
    "butter_b,butter_a=butter(4,1,btype='highpass',fs=sampling_freq)\n",
    "notch_b,notch_a=iirnotch(60,Q=30,fs=sampling_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "preproc_functions = []\n",
    "#Cut to 2 minutes length\n",
    "preproc_functions.append(lambda data, fs: (data[:, :int(duration_recording_mins * 60 * fs)], fs))\n",
    "#Apply butterworth and notch filter\n",
    "preproc_functions.append(lambda data, fs: (filtfilt(butter_b, butter_a, data), fs))\n",
    "preproc_functions.append(lambda data, fs: (filtfilt(notch_b, notch_a, data), fs))\n",
    "#Apply TCP montage technique\n",
    "preproc_functions.append(tcp)\n",
    "preproc_functions.append(lambda data, fs: (resampy.resample(data, fs,sampling_freq,axis=1,filter='kaiser_fast'),sampling_freq))\n",
    "\n",
    "dataset = DiagnosisSet(n_recordings=n_recordings,\n",
    "                           max_recording_mins=max_recording_mins,\n",
    "                           preproc_functions=preproc_functions,\n",
    "                           data_folders=data_folders,\n",
    "                           train_or_eval='train',\n",
    "                           sensor_types=sensor_types)\n",
    "if test_on_eval:\n",
    "    test_dataset = DiagnosisSet(n_recordings=n_recordings,\n",
    "                           max_recording_mins=max_recording_mins,\n",
    "                           preproc_functions=preproc_functions,\n",
    "                           data_folders=data_folders,\n",
    "                           train_or_eval='eval',\n",
    "                           sensor_types=sensor_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y=dataset.load()\n",
    "test_x,test_y=test_dataset.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data=np.array(X)\n",
    "test_data=np.array(test_x)\n",
    "del X,test_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_train_data=np.concatenate([train_data[:,:,:input_time_length] , train_data[:,:,-1:input_time_length-1:-1]])\n",
    "augmented_test_data=np.concatenate([test_data[:,:,:input_time_length] , test_data[:,:,-1:input_time_length-1:-1]])\n",
    "\n",
    "augmented_train_label=np.concatenate([y,y])\n",
    "augmented_test_label=np.concatenate([test_y,test_y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#HDF5 implementation\n",
    "file_names=[]\n",
    "split='train'\n",
    "path=os.path.join(processed_folder,split)\n",
    "for i in range(len(augmented_train_label)):\n",
    "    file_path=f'{path}/{i}.hdf5'\n",
    "    file_names.append(file_path)\n",
    "    with h5py.File(file_path, 'a') as f:\n",
    "        f['x']=augmented_train_data[i]\n",
    "        f['y']=augmented_train_label[i]\n",
    "    #np.savez_compressed(file_path,x=X[i],y=y[i])\n",
    "file_names=pd.Series(file_names)\n",
    "lbs=pd.Series(augmented_train_label)\n",
    "train_dataframe=pd.DataFrame({'name':file_names,'label':lbs})\n",
    "train_dataframe.to_excel(f\"{processed_folder}/{split}.xlsx\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_names=[]\n",
    "split='eval'\n",
    "path=os.path.join(processed_folder,split)\n",
    "for i in range(len(augmented_test_label)):\n",
    "    file_path=f'{path}/{i}.hdf5'\n",
    "    file_names.append(file_path)\n",
    "    with h5py.File(file_path, 'a') as f:\n",
    "        f['x']=augmented_test_data[i]\n",
    "        f['y']=augmented_test_label[i]\n",
    "\n",
    "file_names=pd.Series(file_names)\n",
    "lbs=pd.Series(augmented_test_label)\n",
    "eval_dataframe=pd.DataFrame({'name':file_names,'label':lbs})\n",
    "eval_dataframe.to_excel(f\"{processed_folder}/{split}.xlsx\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WindowDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, excel_path):\n",
    "        super().__init__()\n",
    "        excel_file=pd.read_excel(excel_path)\n",
    "        self.file_names=excel_file['name'].to_numpy(dtype=str)\n",
    "        self.label=excel_file['label'].to_numpy()\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        with h5py.File(self.file_names[index], 'r') as h5_file:\n",
    "            window=h5_file['x']\n",
    "\n",
    "        label=self.label[index]\n",
    "        return window,label\n",
    " \n",
    "    def __len__(self):\n",
    "        return len(self.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The train and test set need to be initialized here\n",
    "test_set=WindowDataset(f'{processed_folder}/eval.xlsx')\n",
    "train_set=WindowDataset(f'{processed_folder}/train.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-5.1844841e-01, -4.4215456e-01, -1.8275532e-01, ...,\n",
       "          1.8161446e+00,  1.6940743e+00,  1.7703682e+00],\n",
       "        [ 1.9992498e+00,  2.1213200e+00,  1.8008858e+00, ...,\n",
       "         -6.3778186e+00, -6.1489372e+00, -6.2404900e+00],\n",
       "        [-2.0138087e+00, -1.9069971e+00, -3.0666642e+00, ...,\n",
       "         -3.9825058e+01, -3.8909531e+01, -3.9016342e+01],\n",
       "        ...,\n",
       "        [ 1.1752758e+00, -1.4950101e+00, -4.5741329e-01, ...,\n",
       "         -1.4644926e+00, -4.1163698e-01, -1.2813872e+00],\n",
       "        [-8.2362396e-01, -4.7267208e-01, -1.5865629e+00, ...,\n",
       "         -7.6258886e-01, -1.0646144e-01, -1.0646144e-01],\n",
       "        [-3.0167554e-02,  2.9026678e-01, -1.0525056e+00, ...,\n",
       "         -1.0604500e+01, -1.0421395e+01, -1.0223031e+01]], dtype=float32),\n",
       " 1)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set.__getitem__(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name='wavenet'\n",
    "criterion=torch.nn.CrossEntropyLoss\n",
    "optimizer_lr=init_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monitor = lambda net: any(net.history[-1, ('valid_accuracy_best','valid_f1_best','valid_loss_best')])\n",
    "cp=Checkpoint(monitor='valid_f1_best',dirname='model',f_params=f'{model_name}best_param.pkl',\n",
    "               f_optimizer=f'{model_name}best_opt.pkl', f_history=f'{model_name}best_history.json')\n",
    "path=f'{model_name}II'\n",
    "classifier = skorch.NeuralNetBinaryClassifier(\n",
    "        model,\n",
    "        optimizer=torch.optim.AdamW,\n",
    "        train_split=predefined_split(test_set),\n",
    "        optimizer__lr=optimizer_lr,\n",
    "        #optimizer__weight_decay=optimizer_weight_decay,\n",
    "        iterator_train=DataLoader,\n",
    "        iterator_valid=DataLoader,\n",
    "        iterator_train__shuffle=True,\n",
    "        iterator_train__pin_memory=True,\n",
    "        iterator_valid__pin_memory=True,\n",
    "        #iterator_train__num_workers=1,\n",
    "        #iterator_valid__num_workers=1,\n",
    "        #iterator_train__persistent_workers=True,\n",
    "        #iterator_valid__persistent_workers=True,\n",
    "        batch_size=batch_size,\n",
    "        device=device,\n",
    "        callbacks=[\"accuracy\",\"f1\",cp,ProgressBar(detect_notebook=True)],\n",
    "        warm_start=True,\n",
    "        )\n",
    "classifier.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test=np.random.rand(3,n_chans,input_time_length)\n",
    "out=classifier.predict(test)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Used to load parameters for ongoing training\n",
    "try:\n",
    "    classifier.load_params(\n",
    "        f_params=f'model/{path}_param.pkl', f_optimizer=f'model/{path}_opt.pkl', f_history=f'model/{path}_history.json')\n",
    "    print(\"Paramters Loaded\")\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.fit(train_set,epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.save_params(\n",
    "    f_params=f'model/{path}_param.pkl', f_optimizer=f'model/{path}_opt.pkl', f_history=f'model/{path}_history.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.load_params(\n",
    "        f_params=f'model/{model_name}best_param.pkl', f_history=f'model/{model_name}best_history.json')\n",
    "print(\"Paramters Loaded\")\n",
    "pred_labels=classifier.predict(test_set)\n",
    "actual_labels=[label[1] for label in test_set]\n",
    "auc=roc_auc_score(actual_labels,classifier.predict_proba(test_set)[:,1])\n",
    "actual_labels=np.array(actual_labels)\n",
    "accuracy=np.mean(pred_labels==actual_labels)\n",
    "tp=np.sum(pred_labels*actual_labels)\n",
    "precision=tp/np.sum(pred_labels)\n",
    "recall=tp/np.sum(actual_labels)\n",
    "f1=2*precision*recall/(precision+recall)\n",
    "\n",
    "print(model_name)\n",
    "print(f\"Accuracy:{accuracy}\")\n",
    "print(f\"F1-Score:{f1}\")\n",
    "print(f\"roc_auc score:{auc}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
